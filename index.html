<!DOCTYPE html>

<html><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta content="IE=5.0000" http-equiv="X-UA-Compatible">
  <meta name="description" content="Falcary's home page"> 
  <link rel="icon" media="(prefers-color-scheme:dark)" href="./assets/imgs/favicon-dark.png" type="image/png" />
  <link rel="icon" media="(prefers-color-scheme:light)" href="./assets/imgs/favicon.png" type="image/png" />
  <link href="./profile.css" rel="stylesheet" type="text/css"> 
  <title>Falcary's Homepage</title> 
  <meta name="GENERATOR" content="MSHTML 11.00.10570.1001">
</head>


<body> 
  <div id="layout-content" style="margin-top: 25px;">
  <table>
    <tbody>
    <tr>
      <td width="670">
        <div id="toptitle">
        <h1>Chaoran Feng</h1></div>
        <h3>ðŸŽ“ Master Student</h3>
        <p><a href="https://www.ece.pku.edu.cn/">School of Electric and Computer Engineering</a>
        <br>Peking University,
        <br>Beijing, China
        <br>
        <br> Email:  
        <a href="mailto:fcr1500583403@gmail.com">fcr1500583403@gmail.com</a>
        <br>
        <br> 
        
        [<a href="https://scholar.google.com/citations?user=Thyo5v4AAAAJ&hl=en">Google Scholar</a>]&nbsp;&nbsp[<a href="https://github.com/SuperFCR">GitHub</a>]
        
      
      </p>
      </td>
      <td>
        <div>
          <img width="270" src="./assets/imgs/bio.jpg" border="0">
        </div>
      </td>
    </tr>
    <tr></tr></tbody>
  </table>
  <div id="layout-content" style="margin-top: 25px;">


<h2>Education</h2>
  <ul>
    <li>
      [2020-2024] ðŸŽ‰ I received my B.E. degree from <a href="https://en.dlut.edu.cn/">Dalian University of Technology</a>, awarded the Outstanding Graduate (Top 3%), ranking <b>1<sup>st</sup></b>/85 for three years (2021-2023).
    </li>
    <li>
      [2024-Now] ðŸ’ª I'm pursuing my M.S. degree in ECE, PKU, supervised by <a href="https://yuanli2333.github.io/">Prof. Li Yuan</a> and <a href="https://scholar.google.com/citations?user=fn6hJx0AAAAJ&hl=en">Prof. Yonghong Tian</a>.
    </li>

  </ul>
    
<h2>Biography</h2>
  <p>I am a first-year M.S. student in ECE at Peking University, holding a B.E. degree from Dalian University of Technology. During my undergraduate studies, I achieved a GPA of 3.8/4.0, ranking <b>1<sup>st</sup></b>/85 in both major and overall scores.
      <br><br> ðŸ“Œ My research interests focus on <i>3D Vision</i>, <i> Neuromorphic Vision</i>, and <i>Generative Model</i>.
  </p>


<h2>News</h2>
<ul>
  <li>
    [2025-06] Two papers accepted by <i>ICCV 2025</i>, <a href="https://arxiv.org/pdf/2405.20224 ">"EvaGaussians"</a> and <a href="https://iccv.thecvf.com/virtual/2025/poster/2554">"Tune-Your-Style"</a>.
  </li>
  <li>
    [2025-03] ðŸ”¥ We release <a href="https://arxiv.org/pdf/2503.23162">"NeuralGS"</a>, a novel framework that effectively adopts the neural field representation to encode the attributes of 3D Gaussians with multiple tiny MLPs.
  </li>
  <li>
    [2025-01] ðŸ”¥ We release <a href="https://arxiv.org/pdf/2501.02807">"AE-NeRF"</a>, a novel framework for 3D reconstruction using event streams under noisy poses and in unbounded larger scenes.
  </li>
  <li>
    [2024-12] Two papers accepted by <i>AAAI 2025</i>.
  </li>
  <li>
    [2024-05] ðŸ”¥ We release <a href="https://arxiv.org/pdf/2405.20224 ">"EvaGaussians"</a>, a first approach that reconstruct sharp static 3D scenes with sparse RGB frames and event streams.
  </li>
  <li>
    [2023-09] One papers accepted by <i>NeurIPS 2023</i>.
  </li>

</ul>



<h2>Selected Projects</h2>
* Equal Contribution &nbsp;&nbsp # Project Lead
<!-- <br><br><tr><td><b><font color="#001F3F">&spades; 3D Reconstruction & Generative Model </font></b></td></tr> -->
<br><br><tr><td><b><font color="#001F3F">&spades; 3D Reconstruction & Generative Model </font></b></td></tr>
<ul>
    <li>
      <div style="margin-top: 30px;"><b>Tune-Your-Style: Intensity-tunable 3D Style Transfer with Gaussian Splatting</b></div>
        <div style="margin-top: 5px;"><a style="color: #777;">Yian Zhao, Rushi Ye, Ruochong Zheng, Zesen Cheng, <u><b>Chaoran Feng*</b></u>, Jiashu Yang, Pengchong Qiao, Chang Liu, Jie Chenâ€ 
        <div style="margin-top: 5px;"><a style="color: #b70505c0;"><i>A novel style transfer framework with 3D Gaussian Splatting.</i></a></div>
        <div style="margin-top: 5px;"><i><b>ICCV 2025</b></i><br></div>
        <div style="margin-top: 5px;">
        [<a href=" , Xinhua Cheng, Wangbo Yu, Junwu Zhang, Yuan Liuâ€ , Xiaoxiao Long, Wenping Wang, Li Yuan">Paper</a>]
        <!-- [<a href="https://github.com/PKU-YuanGroup/NeuralGS">Code ðŸŒŸ150+</a>]</div> -->
      </td>
    </li>
</ul>

<ul>
  <li>
    <div style="margin-top: 30px;"><b>ðŸ”¥ Cycle3D:High-quality and Consistent Image-to-3D Generation via Generation-Reconstruction Cycle</b></div>
      <div style="margin-top: 5px;"><a style="color: #777;">Zhenyu Tang*, Junwu Zhang*, Xinhua Cheng, Wangbo Yu, <u><b>Chaoran Feng</b></u>, Yatian Pang, Bin Lin, Li Yuanâ€ </a></div>
      <div style="margin-top: 5px;"><a style="color: #b70505c0;"><i>The project is about 3d generation used to generation-reconstruction cycle for the unified diffusion process.</i></a></div>
      <div style="margin-top: 5px;"><i><b>AAAI 2025</b></i><br></div>
      <div style="margin-top: 5px;">[<a href="https://arxiv.org/pdf/2407.19548">Paper</a>]
      [<a href="https://github.com/PKU-YuanGroup/Cycle3D">Code ðŸŒŸ200+</a>]</div>
    </td>
  </li>
</ul>

<ul>
    <li>
      <div style="margin-top: 30px;"><b>ðŸ”¥ NeuralGS:Bridging Neural Fields and 3D Gaussian Splatting for Compact 3D Representations</b></div>
        <div style="margin-top: 5px;"><a style="color: #777;">Zhenyu Tang*, <u><b>Chaoran Feng*</b></u>, Xinhua Cheng, Wangbo Yu, Junwu Zhang, Yuan Liuâ€ , Xiaoxiao Long, Wenping Wang, Li Yuanâ€ 
        <div style="margin-top: 5px;"><a style="color: #b70505c0;"><i>A novel framework using neural fields to encode 3D Gaussians with compact MLPs for large-scale scenes.</i></a></div>
        <div style="margin-top: 5px;"><i><b>Arxiv 2025</b></i><br></div>
        <div style="margin-top: 5px;">
        [<a href="https://arxiv.org/pdf/2503.23162">Paper</a>]
        [<a href="https://github.com/PKU-YuanGroup/NeuralGS">Code ðŸŒŸ150+</a>]</div>
      </td>
    </li>
</ul>

<ul>
  <li>
    <div style="margin-top: 30px;"><b>D<sup>2</sup>GS: Deblurring Deformable 3D Gaussian Splatting for Motion-Blurred Causal Videos</b></div>
      <div style="margin-top: 5px;"><a style="color: #777;">  <u><b>Chaoran Feng</b></u>, Jianbin Zhao, Wangbo Yu, Zhenyu Tang, Yuchen Li, Li Yuanâ€ , and Yonghong Tianâ€ 
      <div style="margin-top: 5px;"><a style="color: #b70505c0;"><i>A novel framework to reconstruct High-quality 4D scenes from blurry videos.</i></a></div>
      <div style="margin-top: 5px;"><i><b>Notes: Paper is under review by T-CSVT!</b></i><br></div>
      <div style="margin-top: 5px;">[<a href="./assets/files/D2GS_Deblurring_Deformable_3D_Gaussian_Splatting_for_Motion-Blurred_Causal Vide.pdf">Paper</a>]</div>
    </td>
  </li>
</ul>

<ul>
  <li>
    <div style="margin-top: 30px;"><b>NOFA++: Tuning-free NeRF-based One-shot Facial Avatar Reconstruction</b></div>
      <div style="margin-top: 5px;"><a style="color: #777;">Wangbo Yu, <u><b>Chaoran Feng</b></u>, Yanbo Fanâ€ , Yong Zhang, Xuan Wang, Fei Yin, Yunpeng Bai, Baoyuan Wu, Yan-Pei Cao, Li Yuanâ€ , and Yonghong Tianâ€ </a></div>
      <div style="margin-top: 5px;"><a style="color: #b70505c0;"><i>One-shot 3D facial avatar reconstruction with high fidelity and dynamic reenactment from a single image.</i></a></div>
      <div style="margin-top: 5px;"><i><b>Notes: Paper is under review by T-PAMI!</b></i><br></div>
      <div style="margin-top: 5px;">[<a href="./assets/files/20241015-PAMI_NOFA____NeRF_based_One_shot_Facial_Avatar_Reconstruction_and_beyond.pdf">Paper</a>]</div>
    </td>
  </li>
</ul>


<br><br><tr><td><b><font color="#001F3F">&spades; Neuromorphic Vision </font></b></td></tr>

<ul>
  <li>
    <div style="margin-top: 30px;"><b>AE-NeRF:Augmenting Event-Based Neural Radiance Fields for Non-ideal Conditions and Larger Scenes</b></div>
      <div style="margin-top: 5px;"><a style="color: #777;"><u><b>Chaoran Feng</b></u>, Wangbo Yu, Xinhua Cheng, Zhenyu Tang, Junwu Zhang, Li Yuanâ€ , Yonghong Tianâ€ 
      <div style="margin-top: 5px;"><a style="color: #b70505c0;"><i>The project is about 3d reconstruction with event stream in noisy poses and unbounded larger scenes.</i></a></div>
      <div style="margin-top: 5px;"><i><b>AAAI 2025</b></i><br></div>
      <div style="margin-top: 5px;">[<a href="https://arxiv.org/pdf/2501.02807">Paper</a>]
      [<a href="https://drexubery.github.io/EvaGaussians/">Code</a>]</div>
    </td>
  </li>
</ul>

<ul>
  <li>
    <div style="margin-top: 30px;"><b>EvaGaussians:Event Assisted Gaussian Splatting from Blurry Images</b></div>
      <div style="margin-top: 5px;"><a style="color: #777;">
        Wangbo Yu*, <u><b>Chaoran Feng*</b></u>, Jiye Tang, Jiashu Yang, Zhenyu Tang, Xu Jia, Yuchao Yang, Li Yuanâ€ , Yonghong Tianâ€ 
      <div style="margin-top: 5px;"><a style="color: #b70505c0;"><i>The project is about 3d reconstruction with event stream in noisy poses and unbounded larger scenes.</i></a></div>
      <div style="margin-top: 5px;"><i><b>ICCV 2025</b></i><br></div>
      <div style="margin-top: 5px;">[<a href="https://arxiv.org/abs/2405.20224">Paper</a>]
      [<a href="https://github.com/SuperFCR">Code ðŸŒŸ50+</a>]</div>
    </td>
  </li>
</ul>

<ul>
  <li>
    <div style="margin-top: 30px;"><b>EICIL:Joint Excitatory Inhibitory Cycle Iteration Learning for Deep Spiking Neural Networks</b></div>
      <div style="margin-top: 5px;"><a style="color: #777;">Zihang Shao, Xuanye Fang, Yaxin Li, <u><b>Chaoran Feng</b></u>, Jiangrong Shen, Qi Xuâ€ 
      <div style="margin-top: 5px;"><a style="color: #b70505c0;"><i>The project is about 3d reconstruction with event stream in noisy poses and unbounded larger scenes.</i></a></div>
      <div style="margin-top: 5px;"><i><b>NeurIPS 2023</b></i><br></div>
      <div style="margin-top: 5px;">[<a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/65e876f6a98c6799d0b3145966dd73e2-Paper-Conference.pdf">Paper</a>]</div>
    </td>
  </li>
</ul>


<h2>Award</h2>
<ul>
  <li>
    [2024-06] Outstanding Graduate, Dalian University of Technology (Top <b>3%</b>)
  </li>


  <li>
    [2023-05] 1<sup>st</sup> Prize in the China Robotics and Artificial Intelligence Competition (Ranked <b>1<sup>st</sup></b>/100+)
  </li>

  <li>
    [2022-10] Qubochuan Scholarship, highest scholarship of DUT, awarded to only 10 recipients each year
  </li>
  <li>
    [2021,2022,2023] Nation Scholarship, three times, Dalian University of Technology
  </li>
  <li>
    [2021,2022,2023] Academic Excellence Scholarship (First Class) (Ranked <b>1<sup>st</sup></b>/85), Dalian University of Technology
  </li>
</ul>


<h2>Professional Activities </h2>
<ul>
  <li>
  Conference Reviewer: ICLR[25], CVPR[25], ICCV[25], NeurIPS[24-25], AAAI[25-26], IJCAI[25], ACM MM[25]; 
  </li>
  <li>
  Jounal Reviewer: TCSVT; 
  </li>
</ul>

<ul>
</ul>

</td>
</tr>
</table>

<a href="https://clustrmaps.com/site/1c5bo"  title="Visit tracker">
  <img src="//www.clustrmaps.com/map_v2.png?d=BRf4_qJelQm7X4ukCxJuzTHr88doqXYrA5-S7H0IEWA&cl=ffffff" />
</a>


</body>

</div>
</div>

</body>
</html>
