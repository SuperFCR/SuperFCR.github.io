<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="description" content="Chaoran Feng's homepage">
    <title>Chaoran Feng's Homepage</title>
    <link rel="icon" media="(prefers-color-scheme:dark)" href="./assets/imgs/favicon-dark.png" type="image/png" />
    <link rel="icon" media="(prefers-color-scheme:light)" href="./assets/imgs/favicon.png" type="image/png" />
    <link href="./assets/css/profile.css" rel="stylesheet" type="text/css">
    <meta name="GENERATOR" content="MSHTML 11.00.10570.1001">
</head>

<body> 
  <div id="layout-content" style="margin-top: 25px;">
  <table>
    <tbody>
    <tr>
      <td width="670">
        <div id="toptitle">
        <h1>Chaoran Feng</h1></div>
        <h3><span class="emoji">ğŸ“</span> Master Student</h3>
        <p><a href="https://www.ece.pku.edu.cn/">School of Electric and Computer Engineering</a>
        <br>Peking University,
        <br>Beijing, China
        <br>
        
        <div class="contact-box">
            <img src="./assets/imgs/envelope.png" alt="Email">
            <a href="mailto:fcr1500583403@gmail.com">fcr1500583403@gmail.com</a>

            <img src="./assets/imgs/google.png" alt="Google Scholar">
            <a href="https://scholar.google.com/citations?user=Thyo5v4AAAAJ&hl=en" target="_blank">Google Scholar</a>

            <img src="./assets/imgs/github.png" alt="GitHub">
            <a href="https://github.com/SuperFCR" target="_blank">GitHub</a>
        </div>
        </p>
      </td>

      <td>
        <div>
            <img class="profile-pic" src="./assets/imgs/bio.jpg" alt="Profile Picture">
        </div>
      </td>
    </tr>
    <tr></tr></tbody>
  </table>
  <div id="layout-content" style="margin-top: 25px;">

<h2>Education</h2>
  <ul>
    
    <li>
      [2020-2024] ğŸ‰ I received my B.E. degree from <a href="https://en.dlut.edu.cn/">Dalian University of Technology</a>, awarded the Outstanding Graduate (Top 3%), ranking <b>1<sup>st</sup></b>/85 for three years (2021-2023).
    </li>
    <li>
      [2024-Now] ğŸ’ª I'm pursuing my M.S. degree in ECE, PKU, supervised by <a href="https://yuanli2333.github.io/">Prof. Li Yuan</a> and <a href="https://scholar.google.com/citations?user=fn6hJx0AAAAJ&hl=en">Prof. Yonghong Tian</a>.
    </li>
  </ul>
    
<h2>Biography</h2>
  <p>I am a second-year M.S. student in ECE at Peking University, and achieve a GPA of 3.97/4.0 in Peking University.<br><br>ğŸ“Œ My research interests focus on <i>3D Vision</i>, <i> Neuromorphic Vision</i>, and <i>Generative Model</i>. Due to policies at Pengcheng Lab, some of my code and data are protected and cannot be shared publicly. However, if you'd like to compare with our proposed methods, feel free to reach out to me via email for further discussion.</p>

<h2>News</h2>
<ul>
  
  <li>
    [2025-09]  A paper is accepted by <i>NeurIPS 2025</i>, <a href="https://arxiv.org/pdf/2505.15287">"GS2E"</a>.
  </li>
  <li>
    [2025-07]  A paper is accepted by <i>ACMMM 2025</i>, E-4DGS. The code and paper are coming soon.
  </li>
  <li>
    [2025-06]  Two papers are accepted by <i>ICCV 2025</i>, <a href="https://arxiv.org/pdf/2405.20224 ">"EvaGaussians"</a> and <a href="https://iccv.thecvf.com/virtual/2025/poster/2554">"Tune-Your-Style"</a>.
  </li>
  <li>
    [2025-03] ğŸ”¥ We release <a href="https://arxiv.org/pdf/2503.23162">"NeuralGS"</a>, a novel framework that effectively adopts the neural field representation to encode the attributes of 3D Gaussians with multiple tiny MLPs.
  </li>
  <li>
    [2025-01] ğŸ”¥ We release <a href="https://arxiv.org/pdf/2501.02807">"AE-NeRF"</a>, a novel framework for 3D reconstruction using event streams under noisy poses and in unbounded larger scenes.
  </li>
  <li>
    [2024-12]  Two papers are accepted by <i>AAAI 2025</i>.
  </li>
  <li>
    [2024-05] ğŸ”¥ We release <a href="https://arxiv.org/pdf/2405.20224 ">"EvaGaussians"</a>, a first approach that reconstruct sharp static 3D scenes with sparse RGB frames and event streams.
  </li>
</ul>

<h2>Selected Projects</h2>
* Equal Contribution &nbsp;&nbsp # Project Lead
<br><br><tr><td><b><font color="#001F3F">&spades; 3D Reconstruction & Generative Model </font></b></td></tr>
<ul class="pub-list">
    
    <li>
        <div class="publication-entry">
            <img src="./assets/pipelines/Pipeline_tune_your_style.png" alt="ğŸ¨ Tune-Your-Style: Intensity-tunable 3D Style Transfer with Gaussian Splatting">
            <div>
                <b>ğŸ¨ Tune-Your-Style: Intensity-tunable 3D Style Transfer with Gaussian Splatting</b><br>
                <span style="color: #777;">Yian Zhao, Rushi Ye, Ruochong Zheng, Zesen Cheng, <u><b>Chaoran Feng*</b></u>, Jiashu Yang, Pengchong Qiao, Chang Liu, Jie Chenâ€ </span><br>
                <i style="color: #b70505c0;">A novel style transfer framework with 3D Gaussian Splatting.</i><br>
                <i><b>ICCV 2025</b></i> &nbsp; 
                [<a href="https://iccv.thecvf.com/virtual/2025/poster/2554">Paper</a>]
            </div>
        </div>
    </li>
    <li>
        <div class="publication-entry">
            <img src="./assets/pipelines/Pipeline_cycle3d.png" alt="ğŸ”¥ Cycle3D: High-quality and Consistent Image-to-3D Generation via Generation-Reconstruction Cycle">
            <div>
                <b>ğŸ”¥ Cycle3D: High-quality and Consistent Image-to-3D Generation via Generation-Reconstruction Cycle</b><br>
                <span style="color: #777;">Zhenyu Tang*, Junwu Zhang*, Xinhua Cheng, Wangbo Yu, <u><b>Chaoran Feng</b></u>, Yatian Pang, Bin Lin, Li Yuanâ€ </span><br>
                <i style="color: #b70505c0;">The project is about 3D generation using a generation-reconstruction cycle for a unified diffusion process.</i><br>
                <i><b>AAAI 2025</b></i> &nbsp; 
                [<a href="https://arxiv.org/pdf/2407.19548">Paper</a>] [<a href="https://github.com/PKU-YuanGroup/Cycle3D">Code ğŸŒŸ200+</a>]
            </div>
        </div>
    </li>
    <li>
        <div class="publication-entry">
            <img src="./assets/pipelines/Pipeline_neuralgs.png" alt="ğŸŒ€ NeuralGS: Bridging Neural Fields and 3D Gaussian Splatting for Compact 3D Representations">
            <div>
                <b>ğŸŒ€ NeuralGS: Bridging Neural Fields and 3D Gaussian Splatting for Compact 3D Representations</b><br>
                <span style="color: #777;">Zhenyu Tang*, <u><b>Chaoran Feng*</b></u>, Xinhua Cheng, Wangbo Yu, Junwu Zhang, Yuan Liuâ€ , Xiaoxiao Long, Wenping Wang, Li Yuanâ€ </span><br>
                <i style="color: #b70505c0;">A novel framework using neural fields to encode 3D Gaussians with compact MLPs for large-scale scenes.</i><br>
                <i><b>Arxiv 2025</b></i> &nbsp; 
                [<a href="https://arxiv.org/pdf/2503.23162">Paper</a>] [<a href="https://github.com/PKU-YuanGroup/NeuralGS">Code ğŸŒŸ150+</a>]
            </div>
        </div>
    </li>
    <li>
        <div class="publication-entry">
            <img src="./assets/pipelines/Pipeline_d2gs.png" alt="ğŸ“¦ D<sup>2</sup>GS: Deblurring Deformable 3D Gaussian Splatting for Motion-Blurred Causal Videos">
            <div>
                <b>ğŸ“¦ D<sup>2</sup>GS: Deblurring Deformable 3D Gaussian Splatting for Motion-Blurred Causal Videos</b><br>
                <span style="color: #777;"><u><b>Chaoran Feng</b></u>, Jianbin Zhao, Wangbo Yu, Zhenyu Tang, Yuchen Li, Li Yuanâ€ , and Yonghong Tianâ€ </span><br>
                <i style="color: #b70505c0;">A novel framework to reconstruct high-quality 4D scenes from blurry videos.</i><br>
                <i><b>Notes: Paper is under review by T-CSVT!</b></i> &nbsp; 
                [<a href="./assets/files/D2GS_Deblurring_Deformable_3D_Gaussian_Splatting_for_Motion-Blurred_Causal Vide.pdf">Paper</a>]
            </div>
        </div>
    </li>
    <li>
        <div class="publication-entry">
            <img src="./assets/pipelines/Pipeline_nofa++.png" alt="ğŸ‘¤ NOFA++: Tuning-free NeRF-based One-shot Facial Avatar Reconstruction">
            <div>
                <b>ğŸ‘¤ NOFA++: Tuning-free NeRF-based One-shot Facial Avatar Reconstruction</b><br>
                <span style="color: #777;">Wangbo Yu, <u><b>Chaoran Feng</b></u>, Yanbo Fanâ€ , Yong Zhang, Xuan Wang, Fei Yin, Yunpeng Bai, Baoyuan Wu, Yan-Pei Cao, Li Yuanâ€ , and Yonghong Tianâ€ </span><br>
                <i style="color: #b70505c0;">One-shot 3D facial avatar reconstruction with high fidelity and dynamic reenactment from a single image.</i><br>
                <i><b>Notes: Paper is under review by T-PAMI!</b></i> &nbsp; 
                [<a href="./assets/files/20241015-PAMI_NOFA____NeRF_based_One_shot_Facial_Avatar_Reconstruction_and_beyond.pdf">Paper</a>]
            </div>
        </div>
    </li>
</ul>

<br>
<tr><td><b><font color="#001F3F">&spades; Neuromorphic Vision </font></b></td></tr>
<ul class="pub-list">
  
  <li>
    <div class="publication-entry">
      <img src="./assets/pipelines/Pipeline_gs2e.png" alt="ğŸ¡ GS2E: Gaussian Splatting is an Effective Data Generator for Event Stream Generation">
      <div>
        <b>ğŸ¡ GS2E: Gaussian Splatting is an Effective Data Generator for Event Stream Generation</b><br>
        <span style="color: #777;">Yuchen Li*, <u><b>Chaoran Feng<sup>*,#</sup></b></u>, Zhenyu Tang, Kaiyuan Deng, Wangbo Yu, Yonghong Tianâ€ , Li Yuanâ€ </span><br>
        <i style="color: #b70505c0;">The large-scale event dataset and a novel pipeline to simultate the event stream with 3DGS.</i><br>
        <i><b>NeurIPS 2025 D&B Track</b></i> &nbsp;
        [<a href="https://arxiv.org/pdf/2505.15287">Paper</a>] [<a href="https://github.com/PKU-YuanGroup/GS2E?tab=readme-ov-file">Code</a>] [<a href="https://huggingface.co/datasets/Falcary/GS2E">Datasets</a>]
      </div>
    </div>
  </li>
  <li>
    <div class="publication-entry">
      <img src="./assets/pipelines/Pipeline_e_4dgs.png" alt="ğŸˆ E-4DGS: High-Fidelity Dynamic Reconstruction from the Multi-view Event Cameras">
      <div>
        <b>ğŸˆ E-4DGS: High-Fidelity Dynamic Reconstruction from the Multi-view Event Cameras</b><br>
        <span style="color: #777;"><u><b>Chaoran Feng</b></u>, Zhenyu Tang, Wangbo Yu, Yatian Pang, Yian Zhao, Jianbin Zhao, Li Yuanâ€ , Yonghong Tianâ€ </span><br>
        <i style="color: #b70505c0;">A novel framework to reconstruct high-fidelity scenes with fast-motion event cameras.</i><br>
        <i><b>ACMMM 2025</b></i> &nbsp;
        [<a href="https://arxiv.org/pdf/2508.09912">Paper</a>] [<a href="https://github.com/SuperFCR/E-4DGS">Code</a>] [<a href="https://huggingface.co/datasets/Anonymous-ACMMM-2025-Submission/Anonymous_ACMMM_2025_Submission">Datasets</a>]
      </div>
    </div>
  </li>
  <li>
    <div class="publication-entry">
      <img src="./assets/pipelines/Pipeline_evagaussian.png" alt="âœ¨ EvaGaussians: Event Assisted Gaussian Splatting from Blurry Images">
      <div>
        <b>âœ¨ EvaGaussians: Event Assisted Gaussian Splatting from Blurry Images</b><br>
        <span style="color: #777;">Wangbo Yu*, <u><b>Chaoran Feng*</b></u>, Jiye Tang, Jiashu Yang, Zhenyu Tang, Xu Jia, Yuchao Yang, Li Yuanâ€ , Yonghong Tianâ€ </span><br>
        <i style="color: #b70505c0;">Event-assisted 3D reconstruction from blurry images with noisy poses and dynamic scenes.</i><br>
        <i><b>ICCV 2025</b></i> &nbsp;
        [<a href="https://arxiv.org/abs/2405.20224">Paper</a>] [<a href="https://github.com/SuperFCR">Code ğŸŒŸ50+</a>]
      </div>
    </div>
  </li>
  <li>
    <div class="publication-entry">
      <img src="./assets/pipelines/Pipeline_ae_nerf.png" alt="âš¡ AE-NeRF: Augmenting Event-Based Neural Radiance Fields for Non-ideal Conditions and Larger Scenes">
      <div>
        <b>âš¡ AE-NeRF: Augmenting Event-Based Neural Radiance Fields for Non-ideal Conditions and Larger Scenes</b><br>
        <span style="color: #777;"><u><b>Chaoran Feng</b></u>, Wangbo Yu, Xinhua Cheng, Zhenyu Tang, Junwu Zhang, Li Yuanâ€ , Yonghong Tianâ€ </span><br>
        <i style="color: #b70505c0;">3D reconstruction with event streams under noisy poses and unbounded scenes.</i><br>
        <i><b>AAAI 2025</b></i> &nbsp;
        [<a href="https://arxiv.org/pdf/2501.02807">Paper</a>] [<a href="https://drexubery.github.io/EvaGaussians/">Code</a>]
      </div>
    </div>
  </li>
</ul>

<h2>Award</h2>
<ul>
  
  <li>
    [2025-09]ğŸŒŸ Ping'an Scholarship, Peking University
  </li>
  <li>
    [2024-06]ğŸ“ Outstanding Graduate, Dalian University of Technology (Top <b>3%</b>)
  </li>
  <li>
    [2023-05]ğŸ† 1<sup>st</sup> Prize in the China Robotics and Artificial Intelligence Competition (Ranked <b>1<sup>st</sup></b>/100+)
  </li>
  <li>
    [2022-10]ğŸŒŸ Qubochuan Scholarship, highest scholarship of DUT, awarded to only 10 recipients each year
  </li>
  <li>
    [2021,2022,2023]ğŸŒŸ Nation Scholarship, three times, Dalian University of Technology
  </li>
  <li>
    [2021,2022,2023]ğŸŒŸ Academic Excellence Scholarship (First Class) (Ranked <b>1<sup>st</sup></b>/85), Dalian University of Technology
  </li>
</ul>

<h2>Professional Activities </h2>
<ul>
  <li>
  Conference Reviewer: ICLR[25-26], CVPR[25], ICCV[25], NeurIPS[24-25], AAAI[25-26], IJCAI[25], ACM MM[25]; 
  </li>
  <li>
  Jounal Reviewer: TCSVT; 
  </li>
</ul>

<a href="https://clustrmaps.com/site/1c5bo"  title="Visit tracker">
  <img src="//www.clustrmaps.com/map_v2.png?d=BRf4_qJelQm7X4ukCxJuzTHr88doqXYrA5-S7H0IEWA&cl=ffffff" />
</a>

</body>
</div>
</div>
</body>
</html>